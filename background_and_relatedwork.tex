
\chapter{Background and related work}
\label{cha:related_work}
Before all, time series related terminologies are introduced in more details.
These include distance measurement methods, averaging methods and semi supervised clustering methods in perspective of time series. Apart from the basic ones, I also introduce the variant ones.

\section{Dissimilarity measurement }
Euclidean distance is commonly used in measuring distances between two points, two vectors, etc. It is simple and competitive for many senarios. However, it is not the best choice in some domains, such as time series. Two time series might be sensitive to time axis, even thought they share similar shapes they might not match well in Euclidean distance. A new way of calculating distance between time series is thus proposed, which is DTW(Dynamic Time Wrapping)
\subsection{DTW}
Dynamic time warping (DTW) is a well-known method to measure the distance between time series instances. Unlike Euclidean distance, which is the sum of the distance of instances in each time axis, as shown in \ref{algo:dtw}, DTW finds an optimal alignment between two time series. Intuitively, it uses dynamic programming to map two time series in a way that their peaks and their bottoms are mapped together. In other words, DTW maps their trends instead of their time axis.

\begin{algorithm}[h] \label{algo:dtw}
	\caption{DTW (s: array[0...n], t: array[0...m])}
	\begin{algorithmic}[1]
		\STATE DTW := array[0..n, 0..m]
		\FOR {i := 0 to n}
		\FOR {j := 0 to m}
			\STATE DTW[i, j] := d(s[i], t[j])
		\ENDFOR
		\ENDFOR
		\FOR { i := 1 to n}
		 \STATE DTW[i, 0] += DTW[i-1, 0]
		\ENDFOR
		
		\FOR { j := 1 to n}
		\STATE DTW[0, j] += DTW[0, j-1]
		\ENDFOR
		
		\FOR { i := 1 to n}
		\FOR {j := 1 to m}	
		\STATE DTW[i, j] += min(DTW[i-1, j],									
							DTW[i, j-1],
							DTW[i-1, j-1])
		\ENDFOR	
		\ENDFOR
	   \STATE return DTW[n-1, m-1]						
		
	\end{algorithmic}
\end{algorithm}

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=6cm]{pics/related_work/dtw}
		\caption{DTW}
		\label{fig:dtw}
	\end{minipage}

\end{figure}
\subsection{others (eg. soft, k-shape, in brief)}
\subsection{motivation for selecting DTW}
DTW not only captures amplitude information of time series and computes their distance based on it, but also records how two time series are mapped to each other in the process of wrapping. In other words, the wrapping can be seen as evidence why two time series are similar to each other, or vise versa. DTW itself has good explanation ability.


\section{Averaging methods}
Apart from selecting the proper distance measure method for time series instances, choosing the right averaging method for time series instances is also of much importance. A suitable average of time series should reflect the overall trend of them as well as the peak and bottom message. 
\subsection{DBA}
DTW Barycenter Averaging(DBA) is designed as such that first initialize the average as the medoid of the time series instances, then update the average by averaging the wrapping of other time series instances from the current average for serveral times. Note that the wrapping of the time series instance from the current average is calculated with DTW. 
\subsection{others}
\subsection{motivation for using DBA}
As we can see from \fref{fig:arithmetic} and \fref{fig:dba}, DBA can capture the most important trend of instances, while the characteristic of the original instances are averaged in the time axis so the information about peaks and bottoms are blurred in the arithmetic mean. As a result, I select DBA as the averaging method in this paper. 

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=6cm]{pics/related_work/arithmetic_mean}
		\caption{Arithmetic mean}
		\label{fig:arithmetic}
	\end{minipage}
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=6cm]{pics/related_work/DBA}
		\caption{DBA}
		\label{fig:dba}
	\end{minipage}
\end{figure}

\section{Semi supervised time series clustering methods}

\subsection{Semi-supervised learning}
In supervised learning, we have the labels of all the instances, thus we can check the accuracy of our algorithms by comparing the expected labels and predicted labels of the instances. It enjoys clarity of data and is easy to train. However, labeling all the data is not a trival task, and sometimes it is not easy or even impossible to get the labels of data.

In unsupervised learning, no data is labeled, and algorithms are expected to find patterns on these unlabeled data. It saves the effort of collecting the labels of data. However, it can not reflect any users' export knowledge.

Clustering is subjective in nature. Different users might expect different clustering results given same dataset. In Semi-supervised learning, a small part of data are labeled and most of the data are unlabeled.  Little effort on collecting expert knowledge can contribute greatly to the clustering result. These small part of labeled data not only shape the result of clustering, but also accelerate the process of clustering. 
\subsection{COBRA}
COBRA is a constraint-based semi-supervised clustering method. Contraint-based clustering does not ask the users the labels of the instances directly. Instead, it queries the relation between instance pairs and adds contraints on the instances. There are two types of constraints, must-link and cannot-link. A must-link means that two instances should be in the same cluster, while a cannot-link means that two instances should be in different clusters. Apparently, the target should be asking users as few queries as possible to get a clustering close to the expected one.  

Suppose we have $  N$ instances, to get the relation between all the instance pairs we need $N*(N-1)/2$ quries, so the query number increases exponentionally as the instance number grows. We can reduce the query number by introduing \textit{constraint entailment and transitivity}.  Must-link constraints are transitive:

\begin{center}$must-link(A, B) \wedge must-link(B, C) \Rightarrow must-link(A, C)$\end{center}

\noindent ,and cannot-link constraints have entailment property:

\begin{center}$must-link(A, B) \wedge cannot-link(B, C) \Rightarrow cannot-link(A, C)$\end{center}
 
\noindent As a result, whenever we get a constraint, we can apply the contraint entailment and transitivity to enlarge the constraint set. In this case, we can decrease the query number.

The order the constraints are obtained is important. You can not do contraint expansion if you have two cannot-links, as both transitivity rule and entailment rule can not be used in this case. However, as long as one of the constraint is must-link, contraint expansion is possible. As a result, it would be better if we could get must-links earlier. In other words, users should be queried the most similar pair of instances.

All of the above clustering methods produce very good clustering. Still, they need a large number of queries to reach perfect result. We need a better algorithm to decrease the query number, especially for clustering on large datasets. COBRA is designed for this purpose. COBRA introduces \textit{super-instance}, which is a small group of similar instances. COBRA get super-instances using K-means algorithm, and later only cluster on the representatives of the super-instance. The representative of a super-instance could be its medoid. Like mentioned above, when querying the constraints between pairs of representatives, the closest one is chosen to query first to obtain must-link as early as possible. COBRA keeps querying users and attempting to merge the super-instances until all the links for pairs of representatives in the cluster are cannot-links, which means no super-instances could be merged further. The COBRA algorithm is shown in detail in \ref{algo:cobra}. 


\begin{algorithm}[h] \label{algo:cobra}
	\caption{COBRA}
	\begin{algorithmic}[1]
		
		\STATE to be completed
	\end{algorithmic}
\end{algorithm}

As shown in \fref{fig:contrain-based-level}, we are going to cluster on five dots, A, B, C, D and E. The target clustering is that blue dots A, B and C are in the same cluster, while orange dots D and E are in the same cluster. The lines between instances indicate their relation. Green ones are must-links, while the red ones are cannot-links. The solid lines are the contraints that are needed to queried to get the overall relation. In  \fref{fig:contrain-based-level}a, relations of all instance pairs are queried, so 10 queries are needed. In \fref{fig:contrain-based-level}b, by using constraint transitivity and entailment, the query number is reduced to 6. In \fref{fig:contrain-based-level}c, by further querying the closest pair of instance first, the query number is further decreased to 4. In \fref{fig:contrain-based-level}d, COBRA beats the former three by introducing super-instances and the query number is only 2. We can see that COBRA can substantially reduce the number of queries, which makes it query-efficient. Besides, there is no need to decide the number of clusters beforehand. However, we need to initialize it with the number of super-instances at the very beginning, and this number has an unignorable effect on the result of the clustering. Moreover, COBRA might not obtain perfect result when instances inside a super-instance actually should not be together. I will explain it in more details in \ref{sec:COBRAS}.  

 \begin{figure}
 		\centering
 		\includegraphics[width=1\textwidth]{pics/related_work/cobra1}
 		\caption{different levels of contraint-based semi-supervised clustering}
 		\label{fig:contrain-based-level}

 \end{figure}

\subsection{COBRAS} \label{sec:COBRAS}
Although COBRA can greatly reduce the query number and increase the speed of getting clustering result, there is one obvious defect of COBRA. Once a super-instance is formed, it can only be merged into bigger super-instance, and can never be splitted again. As users, we are asked to provide COBRA with the number of original super-instances so that COBRA can use it in K-means algorithm to split the data at the very beginning. If we fail to designate a good super-instance number, unexpected results might occur. For instance, if we set the original super-instance number to a number that is too small, we might get underclustering, where instances that are not supposed to be in the same group are clustered together. On the contrary, if we set the original super-instance number to a number that is too big, we might get overclustering, where instances that are supposed to be in the same group are splitted away. As a result, getting a clustering of perfect granuity is not always the case in COBRA.

COBRAS is designed to automatically form clusters of better granuity. It refines COBRA by giving chance to split super-instances after it is formed. A main difference between COBRAS and COBRA is that in COBRAS , even if instances that are not expected to be in the same cluster are merged together in some phase, there still exists the chance for them to separate later. In other words, in COBRA there are only merging, but in COBRAS merging and splitting happen alternately,, until the query budget is used up. Another important difference betwen COBRAS and COBRA is that in COBRAS there is no such need to set up an initial super-instance number, as COBRAS can decide it by itself. The COBRAS algorithm is explained in more details in \ref{sec:COBRAS}.

\begin{algorithm}[h] \label{algo:cobras}
	\caption{COBRAS}
	\begin{algorithmic}[1]
		
		\STATE to be completed
	\end{algorithmic}
\end{algorithm}


The learning curve of COBRA and COBRAS are compared in \fref{fig:learning_curve_cobras}. For COBRA, when the number of super-instances is small, as shown in the orange curve, the clustering quality increases quickly at first but can only reach a suboptimal level. When the number of super-instances is big, as shown in the red curve, relatively good clustering result is obtained in the end but it takes more queries to get this result. For COBRAS, as shown in the dotted blue curve, not only optimal clustering result is obtained in the end, but also the clustering performance is already high within fewer queries. 

The clustering result of COBRA and COBRAS are compared in \fref{fig:result_cobras}. From left to right, results of CORBA with 10 initial super-instances, COBRA with 100 initial super-instances, COBRAS are shown. Dots having the same color are clustered into the same group. The difference between images on the upper layer and the lower layer is the query number that is used. We can see that both COBRA with 100 initial super-instances and CORBAS can obtain perfect result in the end. However, COBRAS is more query-efficient as it obtain this result within 36 queries, while COBRA with 100 initial super-instances needs 103 queries. For COBRA with 10 initial super-instances, it stucks after 14 queries and ends in a suboptimal clustering. As we can see in the red circle, the yellow cluster has included a small part which is expected to be in the brown cluster, which is the phenomenon of underclustering. In all, COBRAS beats COBRA for its ability to converge to high-quality clustering in shorter time with fewer queries.  

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{pics/related_work/learning_curve_cobras}
	\caption{learning curves of COBRA and COBRAS}
	\label{fig:learning_curve_cobras}
\end{figure}



\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{pics/related_work/result_cobras}
	\caption{learning curves of COBRA and COBRAS}
	\label{fig:result_cobras}
\end{figure}




\subsection{$  COBRAS^{TS}$ }
COBRAS-TS is the time series version of COBRAS. 
\subsection{others}
\subsection{motivation of selecting COBRAS}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
