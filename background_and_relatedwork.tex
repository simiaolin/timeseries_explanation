
\chapter{Background and related work}
\label{cha:related_work}
Before all, time series-related terminology is introduced in more detail.
These include distance measurement methods, averaging methods, and semi-supervised clustering methods in the perspective of time series. 

\section{Dissimilarity measurement for time series}
Euclidean distance is commonly used in measuring distances between two points, two vectors, etc. It is simple and competitive for many scenarios. However, it is not the best choice in some domains, such as time series. Time series data are sensitive to the time axis. Even though they share similar shapes they might not match well in Euclidean distance because their shifts or speed might not synchronize. A better way of measuring distance between time series that can capture their similarities is Dynamic Time Warping (DTW).
\subsection{DTW}


\begin{figure}[bp]
	\centering
	\begin{minipage}[t]{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{pics/related_work/dtw_vs_normal}
		\caption{DTW VS Euclidean distance \cite{dtw_vs_euclidean}}
		\label{fig:dtw_vs_normal}
	\end{minipage}
	
\end{figure}
Dynamic time warping (DTW) \cite{senin2008dynamic} is a well-known method to measure the distance between time series instances (could be also called sequences). \cref{fig:dtw_vs_normal} gives an intuitive illustration of the difference between Euclidean distance and DTW. \cref{fig:dtw_vs_normal} (a) shows two time series, A and B, respectively. We are interested in their similarity.  \cref{fig:dtw_vs_normal} (b) illustrates how points in A and B are mapped to each other in Euclidean distance. We can see that the element of A at position $ i $ is mapped to the element of B at position $ i $. Unlike Euclidean distance,  \cref{fig:dtw_vs_normal} (c) shows that in DTW, there is no such necessity that the element of A at position $ i $ is mapped to the element of B at position $ i $. Actually in this case the element of A at position $ i $ is mapped to the element of B at position $ i+2 $. DTW is designed in such a way as to find optimal alignment between two time series. Intuitively, it uses dynamic programming to map two time series in a way that similar values, e.g. peaks and troughs are mapped together. 




Before diving into the details of the DTW algorithm, we would like to first introduce some properties of DTW. 

\begin{itemize}
	\item Every index from one time series should be matched with one or more indices from the other time series.
	\item The first index from the first time series should be matched with the first index of the second time series. The same rule holds for the last index from the first time series and the last index from the second time series
	\item The mapping of indices from one time series to the other time series should be monotonically increasing. For example, if $ i < j $ are indices from one time series, and $ k < l $ are indices from the other time series, there is no such mapping like index $ i $ is mapped to index $ l $ and index $ j $ is mapped to index $ k $.
\end{itemize}

\begin{figure}[htb]
	\centering
	\begin{minipage}[t]{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{pics/related_work/dtw_w_all}
		\caption{Warping in DTW}
		\label{fig:dtw_w_all}
	\end{minipage}
\end{figure}

DTW finds the best alignment between two time series while satisfying these preconditions by solving a dynamic programming problem. Suppose there are two sequences, sequence $ S $ of length $ n $ and sequence $ T$ of length $ m $. DTW maintains a two-dimension distance matrix of size $ n * m $, where element with index $ i, j $ records the accumulated distance of the best alignment between subsequence of $ S $ with indices from $ 1 $ to $ i $ and subsequence of $ T $ with indices from $ 1 $ to $ j $. In \cref{fig:dtw_w_all}, we are interested in the accumulated distance between index $ i $ in first sequence and index $ j $ in the second sequence.  At this moment the index i is mapped to the index j, which is shown in the blue line. To meet with the preconditions of DTW mentioned above, there are three possibilities on the choice of the former alignment, index $ i-1 $ mapped to index $ j $, index $ i $ mapped to index $ j-1 $ and index $ i-1 $ mapped to index $ j-1 $, which are shown in red lines. The alignment of the three having the smallest accumulated distance will be chosen. 

Let's look into the \cref{algo:dtw}. From lines 2 to 6, it initializes the matrix with the distance of index pairs in two sequences. From lines 13 to 17, it modifies each element of the matrix by adding the smallest accumulated distance of the three possible former alignments.





\begin{algorithm}[h] 
	\caption{DTW (S: array[1...n], T: array[1...m])}
	\label{algo:dtw}
	\begin{algorithmic}[1]
		\REQUIRE sequence S of length n
		\REQUIRE sequence T of length m	
		\STATE	\textbf{Let} DTW to be a matrix of size $ n * m  $
		\FOR {i = 1 to n}
		\FOR {j = 1 to m}
		\STATE DTW[i, j] := d(S[i], T[j])
		\ENDFOR
		\ENDFOR
		\FOR { i = 2 to n}
		\STATE DTW[i, 1] += DTW[i-1, 1]
		\ENDFOR
		
		\FOR { j = 2 to n}
		\STATE DTW[1, j] += DTW[1, j-1]
		\ENDFOR
		
		\FOR { i = 2 to n}
		\FOR {j = 2 to m}	
		\STATE DTW[i, j] += min(DTW[i-1, j],									
		DTW[i, j-1],
		DTW[i-1, j-1])
		\ENDFOR	
		\ENDFOR
		\STATE return DTW[n, m]						
		
	\end{algorithmic}
\end{algorithm}


We will illustrate DTW with a specific example. In \cref{fig:dtw_ex_1} (a) we are interested in the distance of two time series P and Q. In this case, we use Manhattan distance as the base distance. Intuitively they are similar in shapes, but they are slightly shifted in time from each other. Let's see how DTW does the warping magic to match their similarity and compute a small distance between them. In \cref{fig:dtw_ex_1} (b), when we look at the $ 7th $ index of sequence P (with a value of 6) and the $ 3rd $ index of sequence Q (with a value of 4), we can see that the original Manhattan distance between them is $ |6 - 4| $, which is 2. To get the smallest accumulated distance for them, we first look at the accumulated distance of the three possible alignments before them. The accumulated distance of the $ 6th $ index of P and the $ 2nd $ index of Q is 23. The accumulated distance of the $ 6th $ index of P and the $ 3rd $ index of Q is 14. The accumulated distance of the $ 7th $ index of P and the $ 2nd $ index of Q is 28. The smallest of the three is 14, so the accumulated distance of the $ 7th $ index of P and the $ 3rd $ index of Q is $ 14+2=16$, which is circled in blue. Finally, we can see that the distance between P and Q is 0 if computed by DTW, which indicates the high similarity between P and Q. Besides, we can easily backtrack from the end of the matrix by choosing the smallest accumulated distance step by step, which is shown with a grey background. In this way, we can visualize how P is warped to Q in the whole DTW process.


\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{\textwidth}
		\centering
		\includegraphics[width=\linewidth]{pics/related_work/dtw_ex_1}
		\caption{Euclidean distance \cite{dtw_ex}}
		\label{fig:dtw_ex_1}
	\end{minipage}
\end{figure}


\subsection{Motivation for selecting DTW}
DTW not only captures amplitude information of time series and computes their distance based on it, but also records how two time series are mapped to each other in the process of warping. In other words, the warping can be seen as evidence of why two time series are similar to each other, or vise versa. DTW itself has good explanation ability.


\section{Averaging methods for time series}
Apart from selecting the proper distance measurement method for time series instances, choosing the right averaging method for time series instances is also of much importance. A suitable average of time series should reflect the overall trend of them without missing other important characteristics, such as peaks and troughs. DTW Barycenter Averaging (DBA) \cite{petitjean2011global} is designed for this purpose.
\subsection{DBA}
As can be seen from its name, DTW Barycenter Averaging (DBA) makes use of DTW. It first initializes the average as the medoid of the time series instances, which is the one among all the time series instances that has the smallest sum of distances to all the other instances. After the initialization, DBA updates the average iteratively. \cref{algo:dba} shows how the average is updated at each iteration provided the old average and the sequences. At each iteration, it first computes the DTW between every sequence and the old average. For each coordinate in the old average, in every sequence there are one or more coordinates that are associated with it, DBA updates the coordinate with the barycenter of all the coordinates associated with it. DBA lets DTW refine its association iteratively and can converge the average gradually. DBA outperforms other averaging methods on sequences in a few ways. Firstly, it is a global averaging method, which means it is insensitive to the ordering of the sequences, which makes sure the stability of the average. Secondly, it modifies the coordinates of the average over time, but it doesn't change the length of the average, which is another sign of stability. Thirdly, the within-group sum of squares (WGSS) of DBA is much smaller than other averaging methods. Last but not least, it is more robust to the diversity of sequences, which means its performance is less influenced if sequences are more mixed. 

\cref{algo:second}, \cref{algo:barycenter} and \cref{algo:dba} are taken from \cite{petitjean2011global}.

\begin{algorithm}[h] 
	\caption{second \cite{petitjean2011global}}
	\label{algo:second}
	\begin{algorithmic}[0]
		\REQUIRE $ p = (a, b): couple  $
		\STATE \RETURN $ b $
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h] 
	\caption{barycenter \cite{petitjean2011global}}
	\label{algo:barycenter}
	\begin{algorithmic}[0]
		\REQUIRE $ X_1, ..., X_n $
		\STATE \RETURN $ \frac{X_1 + ... + X_n} {n} $
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
	\caption{DBA  \cite{petitjean2011global}}
	\label{algo:dba}
	\begin{algorithmic}[0]
		\REQUIRE $ \mathrm{C = <C_{1}, ...,C_{T^{'}} >}$ the initial average sequence
		\ \REQUIRE $ \mathrm{S_1 = <S_{1_1}, ...,S_{1_T} >}$ the 1st sequence to average
		
		\vdots
		
		\REQUIRE$ \mathrm{S_n = <S_{n_1}, ...,S_{n_T} >}$ the nth sequence to average
		\noindent	\STATE \textbf{Let} $  T $ be the length of the sequences
		\noindent \STATE \textbf{Let} $ assocTab $ be a table of size $ T^{'} $ containing in each cell a set of coordinates associated to each coordinate of C
		\STATE \textbf{Let} $ m[ \mathrm{T^{'}}, T] $ be a temporary DTW (cost, path) matrix
		\STATE $ assocTab \leftarrow [\emptyset, ...,\emptyset] $
		
		\FOR {$ seq $ in S}
		
		\STATE $ m \leftarrow DTW(C, seq) $
		\STATE $ i \leftarrow T^{'} $
		\STATE $ j \leftarrow T $
		\WHILE {$ i >= 1 $ and $ j >= 1 $}
		\STATE $assocTab[i] \leftarrow assocTab[i] \bigcup seq_j $
		\STATE $(i, j) \leftarrow second(m[i,j])  $
		\ENDWHILE
		\ENDFOR
		
		\FOR { i = 1 to $  \mathrm{T^{'}}$}
		\STATE $ {C_i}^{'} = barycenter(assocTab[i]) $
		\ENDFOR
		\STATE \textbf{return} $ {C_i}^{'}$
	\end{algorithmic}
\end{algorithm}

\FloatBarrier 
\subsection{Motivation for using DBA}\label{bg:dba}
As we can see from \fref{fig:arithmetic} and \fref{fig:dba} \cite{dta_vs_arithmeticmean}, DBA can capture the most important trend of instances, while the characteristic of the original instances are averaged in the time axis so the information about peaks and troughs are blurred in the arithmetic mean. As a result, we select DBA as the averaging method in this thesis.

\begin{figure}[htbp]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{pics/related_work/arithmetic_mean}
		\caption{Arithmetic mean}
		\label{fig:arithmetic}
	\end{minipage}
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{pics/related_work/DBA}
		\caption{DBA}
		\label{fig:dba}
	\end{minipage}
\end{figure}

\section{Semi supervised time series clustering methods}

\subsection{Semi-supervised learning}
In supervised learning, we have the labels of all the instances. It enjoys the clarity of data and is easy to train. However, labeling all the data is not a trivial task, and sometimes it is not easy or even impossible to get the labels of data \cite{saxena2017review}. 

In unsupervised learning, no data is labeled, and algorithms are expected to find patterns on these unlabeled data. It saves the effort of collecting the labels of data. However, it can not reflect any users' expert knowledge.

Clustering is subjective. Different users might expect different clustering results given the same dataset. For example, images of faces can be clustered by gender, or whether they have eyeglasses or not, etc. In Semi-supervised learning, a small part of data are labeled and most of the data are unlabeled.  Little effort in collecting expert knowledge can contribute greatly to the clustering result. These small parts of labeled data not only shape the result of clustering but also accelerate the process of clustering. 
\subsection{COBRA} \label{sec:COBRA}
COBRA \cite{vancraenendonck2018cobra} is a constraint-based semi-supervised clustering method. Constraint-based clustering does not ask the users the labels of the instances directly. Instead, it queries the relation between instance pairs and adds constraints on the instances. There are two types of constraints, must-link, and cannot-link. A must-link means that two instances should be in the same cluster, while a cannot-link means that two instances should be in different clusters. The target should be asking users as few queries as possible to get a clustering close to the expected one.  

Suppose we have $  N$ instances, to get the relation between all the instance pairs we need $N*(N-1)/2$ queries, so the number of queries increases exponentially as the instance number grows. We can reduce the number of queries by introducing \textit{constraint entailment and transitivity}.  Must-link constraints are transitive:

\begin{center}$\mathrm{must\mbox{-}link}(A, B) \wedge \mathrm{must\mbox{-}link}(B, C) \Rightarrow \mathrm{must\mbox{-}link}(A, C)$\end{center} \label{cobra:entailment}

\noindent ,and cannot-link constraints have entailment property:

\begin{center}$\mathrm{must\mbox{-}link}(A, B) \wedge \mathrm{cannot\mbox{-}link}(B, C) \Rightarrow \mathrm{cannot\mbox{-}link}(A, C)$\end{center} \label{cobra:transitivity}

\noindent As a result, whenever we get a constraint, we can apply the constraint entailment and transitivity to enlarge the constraint set. In this case, we can decrease the number of queries.

The order the constraints are obtained is important. You can not do constraint expansion if you have two cannot-links, as both transitivity rule and entailment rule can not be used in this case. However, as long as one of the constraints is a must-link, constraint expansion is possible. As a result, it would be better if we could get must-links earlier. In other words, users should be queried for the most similar pair of instances.

All of the above clustering methods produce very good clustering. Still, they need a large number of queries to reach the perfect result. We need a better algorithm to decrease the number of queries, especially for clustering on large datasets. COBRA is designed for this purpose. COBRA introduces \textit{super-instance}, which is a small group of similar instances. COBRA get super-instances using K-means algorithm, and later only cluster on the representatives of the super-instance. The representative of a super-instance could be its medoid. As mentioned above, when querying the constraints between pairs of representatives, the closest one is chosen to query first to obtain must-link as early as possible. COBRA keeps querying users and attempting to merge the super-instances until all the links for pairs of representatives in the cluster are cannot-links, which means no super-instances could be merged further. The COBRA algorithm is shown in detail in \cref{algo:cobra}. 


\begin{algorithm}[h]
	\caption{Constraint-based Repeated Aggregation (COBRA) \cite{vancraenendonck2018cobra}}
	\label{algo:cobra}
	\begin{algorithmic}[1]
		\REQUIRE $\mathit{D}$ : a dataset, $\mathit{N_s}$ : the number of super-instances
		\ENSURE a clustering of $\mathit{D}$ 
		\STATE  Construct $\mathit{N_s}$ super-instances by over-clustering $\mathit{D}$  using K-means
		\STATE  Initially, each (partial) cluster consists of a single super-instance
		\WHILE  {the clustering changed}
		\STATE Let $\mathit{L}$ be the list of all pairs of partial clusters between which the relation is not known yet, sorted by their pairwise distance
		\FOR {$\mathit{P_1, P_2}$ $\in$ $\mathit{L}$}
		\STATE Query the relation between partial clusters $\mathit{P_1}$  and $\mathit{P_2}$ 
		\IF {a must-link relation is obtained}
		\STATE merge $\mathit{P_1}$ and $\mathit{P_2}$  into a new partial cluster
		\STATE break
		\ENDIF
		\ENDFOR
		\ENDWHILE
		\RETURN the current clustering
	\end{algorithmic}
\end{algorithm}

As shown in \fref{fig:contrain-based-level}, we are going to cluster on five dots, A, B, C, D, and E. The target clustering is that blue dots A, B, and C are in the same cluster, while orange dots D and E are in the same cluster. The lines between instances indicate their relation. Green ones are must-links, while the red ones are cannot-links. The solid lines are the constraints that are needed to query to get the overall relation. In  \fref{fig:contrain-based-level} (a), relations of all instance pairs are queried, so 10 queries are needed. In \fref{fig:contrain-based-level} (b), by using constraint transitivity and entailment, the number of queries is reduced to 6. In \fref{fig:contrain-based-level} (c), by further querying the closest pair of instances first, the number of queries is further decreased to 4. In \fref{fig:contrain-based-level} (d), COBRA beats the former three by introducing super-instances and the number of queries is only 2. We can see that COBRA can substantially reduce the number of queries, which makes it query-efficient. Besides, there is no need to decide the number of clusters beforehand. However, we need to initialize it with the number of super-instances at the very beginning, and this number has an unignorable effect on the result of the clustering. Moreover, COBRA might not obtain perfect results when instances inside a super-instance actually should not be together. We will explain it in more detail in \cref{sec:COBRAS}.  

\begin{figure}
	\centering
	\includegraphics[width=1\textwidth]{pics/related_work/cobra1}
	\caption{Different levels of contraint-based semi-supervised clustering \cite{vancraenendonck2018cobra}}
	\label{fig:contrain-based-level}
	
\end{figure}

\subsection{COBRAS} \label{sec:COBRAS}
Although COBRA can greatly reduce the number of queries and increase the speed of getting clustering results, there is one obvious defect of COBRA. Once a super-instance is formed, it can only be merged into a bigger super-instance, and can never be split again. As users, we are asked to provide COBRA with the number of original super-instances so that COBRA can use it in the K-means algorithm to split the data at the very beginning. If we fail to designate a good number of super-instances, unexpected results might occur. For instance, if we set the original number of super-instances to a number that is too small, we might get under clustering, where instances that are not supposed to be in the same group are clustered together. On the contrary, if we set the original number of super-instances to a number that is too big, we might get over clustering, where instances that are supposed to be in the same group are split away. As a result, getting a clustering of perfect granularity is not always the case in COBRA.

COBRAS \cite{van2018cobras} is designed to automatically form clusters of better granularity
. It refines COBRA by giving chance to split super-instances after it is formed. The main difference between COBRAS and COBRA is that in COBRAS, even if instances that are not expected to be in the same cluster are merged in some phase, there still exists the chance for them to separate later. In other words, in COBRA there are only merging, but in COBRAS merging and splitting happen alternately, until the query budget is used up. Another important difference between COBRAS and COBRA is that in COBRAS there is no such need to set up an initial number of super-instances, as COBRAS can decide it by itself. 


A COBRAS procedure is elaborated in \fref{fig:cobras_procedure}. At the very beginning, in subgraph A there is only one super-instance $ \mathrm{S_0} $ containing all the instances, and there is one cluster $ \mathrm{C_0} $ containing super-instance $ \mathrm{S_0} $.  At the first iteration, in subgraph B COBRAS first splits the biggest super-instance, which is $ \mathrm{S_0} $, into four small super-instances, $ \mathrm{S_1} $, $ \mathrm{S_2} $, $ \mathrm{S_3} $ and $ \mathrm{S_4} $, respectively. New clusters are formed with these new super-instances, which are  $ \mathrm{C_1} $,  $ \mathrm{C_2} $,  $ \mathrm{C_3} $ and  $ \mathrm{C_4} $, correspondly. Later in subgraph C a merging procedure is executed on these clusters just like in COBRA. At the second iteration, same things happen again. In subgraph D COBRAS first splits the biggest super-instance, which is now $ \mathrm{S_2} $, into two super-instances, $ \mathrm{S_5} $ and $ \mathrm{S_6} $ and forms new clusters $ \mathrm{C_7} $ and $ \mathrm{C_8} $. Later in subgraph E the merging happens based on the constraints and now $ \mathrm{C_5} $ and $ \mathrm{C_7} $ are grouped while $ \mathrm{C_6} $ and $ \mathrm{C_8} $ are grouped. Super-instance $ \mathrm{S_5} $ and $ \mathrm{S_6} $ are in the same cluster but later they are refined and splitted into different clusters, which illustrates COBRAS' ability to finetune the granuity of the clusters.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{pics/related_work/cobras_procedure}
	\caption{An illustration of COBRAS procedure \cite{van2018cobrasts}}
	\label{fig:cobras_procedure}
\end{figure}

More details about COBRAS can be found in \cref{algo:cobras}.
\begin{algorithm}[h]
	\caption{Constraint-based Repeated Aggregation and Splitting (COBRAS) \cite{van2018cobras}}
	\label{algo:cobras}
	\hspace*{\algorithmicindent} \textbf{$\mathcal{X}$: a dataset, $\mathit{q}$: a query limit} \\
	\hspace*{\algorithmicindent} \textbf{$\mathcal{C}$: a clustering of $\mathit{D}$} 
	\begin{algorithmic}[1]
		\STATE  $\mathit{ML = \emptyset, CL = \emptyset}$
		\STATE $\mathit{S}$ = \{$\mathcal{X}$\}, $\mathit{C = \{S\}}$, $\mathcal{C}$ = {$\mathit\{C\}$}
		\WHILE {|$\mathit{ML}$| + |$\mathit{CL}$| < $\mathit{q}$}
		\STATE $\mathit{S_{split}, C_{origin}} = \argmax_{\mathit{S \in C, C \in} \mathcal{C}} |\mathit{S}|$
		\STATE $\mathit{k,ML,CL}$ = determineSplitLevel($\mathit{S_{split} , ML, CL}$)
		\STATE $\mathit{S_{new_1}, ... , S_{new_k}}$ = K-means($\mathit{S_{split}, k}$)
		\STATE $\mathit{C_{origin} = C_{origin}  \setminus \{S_{spit}\}}$
		\STATE $\mathcal{C}$ = $\mathcal{C}$ $ \bigcup $\ \{\{$\mathit{S_{new_1}}$\}, ...,\{$\mathit{S_{new_k}}$\}\}
		\STATE $\mathcal{C}$, $\mathit{ML, CL}$ = COBRA($\mathcal{C}$, $\mathit{ML, CL}$ )
		\ENDWHILE 
		\RETURN  C
	\end{algorithmic}
\end{algorithm}

\FloatBarrier
The learning curve of COBRA and COBRAS are compared in \fref{fig:learning_curve_cobras}. For COBRA, when the number of super-instances is small, as shown in the orange curve, the clustering quality increases quickly at first but can only reach a suboptimal level. When the number of super-instances is big, as shown in the red curve, a relatively good clustering result is obtained in the end but it takes more queries to get this result. For COBRAS, as shown in the dotted blue curve, not only optimal clustering result is obtained in the end, but also the clustering performance is already high within fewer queries.  

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.5\textwidth]{pics/related_work/learning_curve_cobras}
	\caption{Learning curves of COBRA and COBRAS \cite{van2018cobras}}
	\label{fig:learning_curve_cobras}
\end{figure}


The clustering result of COBRA and COBRAS are compared in \fref{fig:result_cobras}. From left to right, results of CORBA with 10 initial super-instances, COBRA with 100 initial super-instances, COBRAS are shown. Dots having the same color are clustered into the same group. The difference between images on the upper layer and the lower layer is the number of queries that are used. We can see that both COBRA with 100 initial super-instances and CORBAS can obtain the perfect result in the end. However, COBRAS is more query-efficient as it obtains this result within 36 queries, while COBRA with 100 initial super-instances needs 103 queries. For COBRA with 10 initial super-instances, it stucks after 14 queries and ends in a suboptimal clustering. As we can see in the red circle, the yellow cluster has included a small part that is expected to be in the brown cluster, which is the phenomenon of under-clustering. In all, COBRAS beats COBRA for its ability to converge to high-quality clustering in a shorter time with fewer queries.  

\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\textwidth]{pics/related_work/result_cobras}
	\caption{Clustering results of COBRA and COBRAS \cite{van2018cobras}}
	\label{fig:result_cobras}
\end{figure}


\subsection{$ \mathrm{COBRAS^{TS}} $}
Clustering on time series data is more subjective. Several elements in time series data are important, such as time scale, amplitude scale, number of peaks, etc. Different users might focus differently on those elements. Besides, some users might pay more attention to the monotonic behavior of the time series data, while other users might pay more attention to the periodic behavior of the time series data. Sometimes users might be interested in whether there are specific patterns in the time series data. Thus, using semi-supervised learning to exploit users' expert knowledge is very important in time series clustering.

We had discussed COBRAS's ability to obtain perfect clustering efficiently as a semi-supervised clustering method. However, when it comes to time series data, things are not that simple. COBRAS is not inherently suitable for time series data mainly for two reasons. The first one is that COBRAS uses Euclidean distance, which is not the start-of-art distance measure for time series data. The second one is that COBRAS uses the K-means algorithm to split super-instances, which is again not that suitable for time series data. The good thing is that we can still make good use of COBRAS by keeping its main idea and injecting the distance measure and the clustering method that are suitable for time series data. It is how $ \mathrm{COBRAS^{DTW}} $ \cite{van2018cobrasts} is introduced. $ \mathrm{COBRAS^{DTW}} $ replaces the Euclidean distance with the DTW mentioned above and uses spectral clustering to split super-instances. As it is COBRAS specifically for time series data, we also refer to it $ \mathrm{COBRAS^{TS}} $.

%\begin{algorithm}[h] 
%	\caption{$ \mathrm{COBRAS^{TS}} $}
%	\label{algo:cobrasts}
%	\begin{algorithmic}[1]
%	\end{algorithmic}
%\end{algorithm}

\FloatBarrier
\section{Literature review on the explanation of time series clustering}
We also do a literature review on the explanation of time series clustering. 

In \cite{van1999cluster} Neth. Energy Research Foundation (ECN) proposed a calendar view of the clustering of the power demand of ECN in 1988. One the right of \cref{fig:power_demand_by_ecn} is the average sequence per cluster, showing the demand of power at different times of a day. On the left of \cref{fig:power_demand_by_ecn} there is a calendar, and each day in it has the same color as the cluster it belongs to. For instance, dates with dark blue color in the calendar are mainly weekends, and the average sequence in dark blue shows low power demand throughout the whole day. It indicates that the power demand in ECN is low on weekends in 1988, which is in line with common sense.
The calendar view of the clustering result is very clear and easy to understand. However, it is a method well designed for the calendar-based time series data and thus is not that general. Even though, its idea of representing a cluster with its average sequence is worthy of reference.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.95\textwidth]{pics/related_work/power_demand_by_ecn}
	\caption{Calendar view of the power demand in Neth. Energy Research Foundation (ECN) in 1988 \cite{van1999cluster}}
	\label{fig:power_demand_by_ecn}
\end{figure}



\FloatBarrier
Changing time series to other forms is also a popular way of interpreting time series instances. In \cref{fig:gene_bitmap} \cite{kumar2005time} the gene sequences of mitochondrial DNA of four animals are transformed into bitmaps. The more similar the bitmaps are, the more similar the gene sequences are. The top one on the left is the gene bitmap of chimpanzees, while the bottom one on the left is the gene of humans. The top one on the right is the gene bitmap of Indian elephants, while the bottom one on the right is that of Africa elephants. We can see that the left two are similar, and the right two are similar, which illustrates the fact that humans are similar to chimpanzees, while Indian elephants are similar to Africa elephants. Transforming time series into bitmap is especially useful for long and discrete sequences because it provides a nice way of viewing the sequences. However, for time series data that are already visualization friendly, there is no need to add the burden of transforming them into other forms. 


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pics/related_work/gene_sequence}
	\caption{The gene sequences of mitochondrial DNA of four animals \cite{kumar2005time}}
	\label{fig:gene_bitmap}
\end{figure}


\FloatBarrier
Explaining the clustering results hierarchically is quite popular \cite{kumar2005time,sulistiovizts}. The example in \cref{fig:hierarchy_and_bitmap} is quite intuitive.  The most similar sequences are linked in the left-most layer, and step by step forming the hierarchy through linking on top of the linked sequences. Actually, it is similar to the merging step of $ \mathrm{COBRAS^{TS}} $. However, it can not display the constraints obtained during $ \mathrm{COBRAS^{TS}} $ such as must-links and cannot-links. In $ \mathrm{COBRAS^{TS}} $, two sequences that are in the same cluster might not have high similarity in terms of distance. They might be in the same cluster because of a must-link provided by users, which is the expert knowledge that is subjective and hard to find a criterion to measure. Even though, it gives us an insight into linking sequences to illustrate their relation.


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.6\textwidth]{pics/related_work/hierarchy_and_bitmap}
	\caption{An example of explaining time series clustering in a hierarchical way \cite{kumar2005time}}
	\label{fig:hierarchy_and_bitmap}
\end{figure}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
