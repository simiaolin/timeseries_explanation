\chapter{Introduction}
\label{cha:intro}
The goal of semi-supervised learning is to combine the opinions of experts with existing information. COBRAS \cite{van2018cobras} the most query-efficient semi-supervised clustering method. It has a time series-based version, $ \mathrm{COBRAS^{TS}} $ \cite{van2018cobrasts}.  Clustering time series data can be of significant value in several scenarios in our daily life, such as clustering on the energy consumption data, electrocardiogram data, etc. However, how the clustering result is obtained is a black box or at least a gray box to end-users. The resulting clustering is difficult to interpret and evaluate, especially for large datasets. This is a significant drawback for semi-supervised systems where experts can contribute to the result but are unable to interpret or evaluate it. We study here how the semi-supervised clustering result can be explained and interpreted. Two benefits are doing so. Apart from providing end-users a more intuitive way of understanding the result, one can convince the experts who give opinions during the clustering process that the result is truly partly based on their knowledge.

\textbf{DTW.} DTW \cite{senin2008dynamic} is a method for computing the alignment and distance between time series instances. Unlike Euclidean distance, DTW computes the best possible alignment between two time series of respective lengths $  n $ and  $  m $  by first computing the $ n * m $ pairwise distance matrix between these points and then solving a dynamic program. The most important characteristic of DTW is its invariance against warping in the time axis. It can capture alternations between leading and lagging relationships of time series.  In other words, if two time series have similar trends but do not correspond to each other in the time axis, DTW will do a warping, making sure that similar trends inside them are mapped to each other. 

\textbf{DBA.} DBA \cite{petitjean2011global} is an averaging method for time series data. It does not calculate the average by the Euclidean distance. Instead, it uses the DTW mentioned above. Compared to the arithmetic mean, DBA preserves the ability of DTW, identifying time shifts. 

\textbf{COBRA.} COBRA \cite{vancraenendonck2018cobra} is a constraint-based semi-supervised clustering method. It allows users to give feedback on the relationship of instances. By introducing the concept of \textbf{super-instance} which are a group of similar instances, and only execute queries on the super-instance, COBRA can substantially reduce the number of queries, which makes it query-efficient.

\textbf{COBRAS.} COBRAS \cite{van2018cobras} is designed to automatically form clusters of better granularity. It refines COBRA by giving chance to split super-instances after they are formed. COBRAS beats COBRA for its ability to converge to clustering with better-grained levels of granularity in a shorter time with fewer queries. 

\textbf{$ \mathbf{COBRAS^{TS}} $.} $ \mathrm{COBRAS^{TS}} $ \cite{van2018cobrasts} is COBRAS with respect to time series.

\textbf{$ \mathbf{COBRAS^{DTW}} $.} $ \mathrm{COBRAS^{DTW}} $ is one of the implementations of the $ \mathrm{COBRAS^{TS}} $. As it uses DTW as its distance measure and DTW itself has good explainabilty, throughout the theis we explain the clustering result of $ \mathrm{COBRAS^{DTW}} $. When we refer $ \mathrm{COBRAS^{TS}} $ in this thesis, we are refering to the implementation $ \mathrm{COBRAS^{DTW}} $.

\textbf{Our contributions.} In this thesis, we propose two new ways to explain the time series clustering. 

Our first idea is to provide an overview of the clustering result by showing the properties of time series instances inside one cluster. We mainly show their DBA and deviation. Considering the instances are time series instances, we propose new types of deviation that can capture the alignment information of the time series data. The idea is new and it helps users build insight into the fact that DTW sacrifices the similarities of time series instances on the X-axis (time-axis) to win similarities of time series instances on the Y-axis (value-axis).

We not only illustrate the clustering result of $ \mathrm{COBRAS^{TS}} $ from a general perspective but also from a specific perspective. Our second idea is to show the relationship chain between two specific instances with constraint information provided by users to explain why they have been clustered together or apart. Moreover, the way we chain two time series instances with constraints is not designed specifically for time series data. It can also be applied in other variants of COBRAS as long as the instances are visually friendly. The idea of chaining two instances with the constraints can be also used in explaining other constraint-based semi-supervised learning.

\textbf{Structure}. After providing background material in chapter \ref{cha:related_work} and problem definition in chapter \ref{cha:problem_definition}, We introduce in chapter \ref{cha:properties_of_a_cluster} different types of deviation for time series instances in one cluster. We follow in chapter \ref{cha:cluster_membership}  illustrating the relationship chain between two time series instances with the constraints obtained in the process of $ \mathrm{COBRAS^{TS}}$. We close this paper in chapter \ref{cha:conclusion} with the pros and cons of my ways of explaining time-series clusters and show potential applications.
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
